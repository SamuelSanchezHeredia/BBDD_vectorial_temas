"""
Base de datos vectorial con Pinecone + FAISS + HuggingFace (sentence-transformers).
Lee el PDF Saberes_2ESO.pdf, genera embeddings y los almacena en Pinecone (nÃºcleo)
y FAISS (motor de bÃºsqueda local).

Arquitectura hÃ­brida:
  - Pinecone: almacenamiento persistente y centralizado (fuente de verdad).
  - FAISS: bÃºsqueda local ultrarrÃ¡pida y offline.

Estrategia de chunking semÃ¡ntico:
  1. Detecta secciones por encabezados (asignaturas, trimestres, tÃ­tulos).
  2. Dentro de cada secciÃ³n agrupa pÃ¡rrafos hasta alcanzar MAX_CHUNK_CHARS.
  3. Si un pÃ¡rrafo supera MAX_CHUNK_CHARS, lo divide por oraciones completas.
  4. Nunca corta palabras a mitad. Cada chunk hereda secciÃ³n y pÃ¡gina de origen.
"""

import os
import re
import sys
import time
import json
import fitz  # PyMuPDF
import numpy as np
import faiss
from dotenv import load_dotenv
from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ConfiguraciÃ³n
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INDEX_NAME = "saberes-2eso"
EMBEDDING_MODEL = "paraphrase-multilingual-MiniLM-L12-v2"  # 384 dims, multilingÃ¼e (espaÃ±ol)
EMBEDDING_DIM = 384
MAX_CHUNK_CHARS = 800   # mÃ¡ximo de caracteres por chunk
MIN_CHUNK_CHARS = 20    # descartar chunks demasiado pequeÃ±os (ruido)
PDF_PATH = os.path.join(os.path.dirname(__file__), "Saberes_2ESO.pdf")
FAISS_DIR = os.path.join(os.path.dirname(__file__), "faiss_index")
FAISS_INDEX_PATH = os.path.join(FAISS_DIR, "index.faiss")
FAISS_METADATA_PATH = os.path.join(FAISS_DIR, "metadata.json")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Funciones auxiliares
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_env():
    """Carga y valida las variables de entorno."""
    load_dotenv(os.path.join(os.path.dirname(__file__), ".env"))
    api_key = os.getenv("PINECONE_API_KEY")
    if not api_key or api_key.startswith("tu_clave"):
        print("âŒ ERROR: Falta la PINECONE_API_KEY en el archivo .env")
        sys.exit(1)
    return api_key


def extract_text_from_pdf(path: str) -> list[dict]:
    """Extrae texto del PDF pÃ¡gina a pÃ¡gina usando PyMuPDF."""
    doc = fitz.open(path)
    pages = []
    for i, page in enumerate(doc):
        text = page.get_text()
        if text.strip():
            pages.append({"text": text, "page": i + 1})
    doc.close()
    return pages


def is_heading(line: str) -> bool:
    """
    Detecta si una lÃ­nea es un encabezado de secciÃ³n (asignatura o tÃ­tulo principal).
    Solo marca como heading los nombres de asignaturas y tÃ­tulos del documento,
    NO los sub-apartados como '1.Âº trimestre' que son contenido dentro de la secciÃ³n.
    """
    line = line.strip()
    if not line or len(line) > 60:
        return False
    # Nombres de asignaturas conocidas
    patterns = [
        r"^(Lengua Castellana|MatemÃ¡ticas|Ciencias Naturales|Ciencias Sociales|"
        r"Historia|GeografÃ­a e Historia|BiologÃ­a|FÃ­sica y QuÃ­mica|"
        r"InglÃ©s|FrancÃ©s|Segunda Lengua|EducaciÃ³n FÃ­sica|TecnologÃ­a|"
        r"MÃºsica|PlÃ¡stica|Artes PlÃ¡sticas|ReligiÃ³n|Ã‰tica|FilosofÃ­a|"
        r"EconomÃ­a|InformÃ¡tica|LatÃ­n|Literatura|Valores CÃ­vicos|"
        r"EducaciÃ³n en Valores)",
        r"^Saberes bÃ¡sicos",                          # tÃ­tulo principal del documento
    ]
    for pattern in patterns:
        if re.match(pattern, line, re.IGNORECASE):
            return True
    return False


def detect_trimester(text: str) -> str | None:
    """
    Detecta si el texto EMPIEZA con una marca de trimestre (p.ej. '1.Âº trimestre').
    Retorna '1.Âº trimestre', '2.Âº trimestre', '3.Âº trimestre' o None.
    """
    m = re.match(r"^\s*(\d)\.?[Â°Âºo]?\s*trimestre\b", text, re.IGNORECASE)
    if m:
        n = m.group(1)
        return f"{n}.Âº trimestre"
    return None


def split_by_trimesters(text: str) -> list[tuple[str, str]]:
    """
    Divide un texto que contiene marcas de trimestre inline en fragmentos.
    Retorna lista de tuplas (nombre_trimestre, contenido).
    Si no hay marcas de trimestre, retorna [("General", texto_completo)].
    Ejemplo:
      '1.Âº trimestre NÃºmeros ... 2.Âº trimestre Ãlgebra ...'
      â†’ [('1.Âº trimestre', 'NÃºmeros ...'), ('2.Âº trimestre', 'Ãlgebra ...')]
    """
    # PatrÃ³n que captura las marcas de trimestre inline
    pattern = r"(\d\.?[Â°Âºo]?\s*trimestre)\s*"
    parts = re.split(pattern, text, flags=re.IGNORECASE)
    # parts alterna: [texto_antes, marca1, texto1, marca2, texto2, ...]

    if len(parts) <= 1:
        # Sin marcas de trimestre
        return [("General", text.strip())]

    result = []
    # Si hay texto antes de la primera marca, se asigna como "General"
    pre_text = parts[0].strip()
    if pre_text:
        result.append(("General", pre_text))

    # Recorrer pares (marca, contenido)
    for i in range(1, len(parts), 2):
        raw_marker = parts[i].strip()
        content = parts[i + 1].strip() if i + 1 < len(parts) else ""
        # Normalizar nombre del trimestre
        m = re.match(r"(\d)", raw_marker)
        trimester_name = f"{m.group(1)}.Âº trimestre" if m else raw_marker
        result.append((trimester_name, content))

    return result


def split_by_sentences(text: str, max_chars: int) -> list[str]:
    """
    Divide un texto largo en fragmentos respetando oraciones completas.
    Corta por '. ', '.\n', '? ', '! ' sin partir palabras.
    """
    # Separar por fin de oraciÃ³n manteniendo el separador
    sentences = re.split(r'(?<=[.?!])\s+', text.strip())
    fragments = []
    current = ""
    for sentence in sentences:
        if len(current) + len(sentence) + 1 <= max_chars:
            current = (current + " " + sentence).strip()
        else:
            if current:
                fragments.append(current)
            # Si la oraciÃ³n sola supera el lÃ­mite, la aÃ±ade de todas formas
            # (no se puede dividir sin perder sentido)
            current = sentence
    if current:
        fragments.append(current)
    return fragments


def split_into_chunks(pages: list[dict]) -> list[dict]:
    """
    Chunking semÃ¡ntico en 4 niveles:
      1. Detecta encabezados â†’ marca inicio de nueva secciÃ³n (asignatura).
      2. Detecta marcas de trimestre (inline o al inicio) â†’ chunk por trimestre.
      3. Agrupa pÃ¡rrafos del mismo trimestre/secciÃ³n hasta MAX_CHUNK_CHARS.
      4. Si un pÃ¡rrafo supera MAX_CHUNK_CHARS, lo divide por oraciones.
    Cada chunk incluye: texto, pÃ¡gina, secciÃ³n, trimestre.
    """
    chunks = []

    def flush_chunk(text: str, section: str, trimester: str, page: int):
        """AÃ±ade el chunk a la lista si supera el mÃ­nimo de caracteres."""
        text = text.strip()
        if len(text) >= MIN_CHUNK_CHARS:
            # Prefijar contexto de secciÃ³n/trimestre para mejor embedding
            prefix = f"[{section}]" if section != "General" else ""
            if trimester != "General":
                prefix += f" [{trimester}]" if prefix else f"[{trimester}]"
            enriched_text = f"{prefix} {text}".strip() if prefix else text
            chunks.append({
                "text": enriched_text,
                "section": section,
                "trimester": trimester,
                "page": page,
            })

    def process_paragraph(para: str, section: str, trimester: str, page: int,
                          current_chunk: str) -> tuple[str, str]:
        """
        Procesa un pÃ¡rrafo: si contiene marcas de trimestre inline, divide
        y emite chunks separados. Retorna (chunk_acumulado, trimestre_actual).
        """
        tri_parts = split_by_trimesters(para)

        # Si no hay trimestres, el pÃ¡rrafo es texto normal
        if len(tri_parts) == 1 and tri_parts[0][0] == "General":
            text = tri_parts[0][1]
            if len(current_chunk) + len(text) + 2 <= MAX_CHUNK_CHARS:
                current_chunk = (current_chunk + "\n\n" + text).strip()
            else:
                flush_chunk(current_chunk, section, trimester, page)
                if len(text) > MAX_CHUNK_CHARS:
                    for frag in split_by_sentences(text, MAX_CHUNK_CHARS):
                        flush_chunk(frag, section, trimester, page)
                    current_chunk = ""
                else:
                    current_chunk = text
            return current_chunk, trimester

        # Hay marcas de trimestre: flush lo acumulado y emitir un chunk por trimestre
        flush_chunk(current_chunk, section, trimester, page)
        current_chunk = ""
        last_tri = trimester

        for tri_name, tri_content in tri_parts:
            if tri_name != "General":
                last_tri = tri_name
            if tri_content:
                if len(tri_content) > MAX_CHUNK_CHARS:
                    for frag in split_by_sentences(tri_content, MAX_CHUNK_CHARS):
                        flush_chunk(frag, section, last_tri, page)
                else:
                    flush_chunk(tri_content, section, last_tri, page)

        return "", last_tri

    current_section = "General"
    current_trimester = "General"
    current_chunk = ""
    current_page = 1

    for page_data in pages:
        page_num = page_data["page"]
        lines = page_data["text"].split("\n")
        paragraph_buffer = ""

        for line in lines:
            stripped = line.strip()

            if not stripped:
                # LÃ­nea vacÃ­a â†’ fin de pÃ¡rrafo
                if paragraph_buffer.strip():
                    para = paragraph_buffer.strip()
                    paragraph_buffer = ""
                    current_chunk, current_trimester = process_paragraph(
                        para, current_section, current_trimester, page_num, current_chunk
                    )
                    current_page = page_num
                continue

            if is_heading(stripped):
                # Guardar lo acumulado antes de cambiar de secciÃ³n
                if paragraph_buffer.strip():
                    para = paragraph_buffer.strip()
                    paragraph_buffer = ""
                    current_chunk, current_trimester = process_paragraph(
                        para, current_section, current_trimester, page_num, current_chunk
                    )
                flush_chunk(current_chunk, current_section, current_trimester, current_page)
                current_chunk = ""
                current_section = stripped
                current_trimester = "General"
                current_page = page_num
            else:
                paragraph_buffer = (paragraph_buffer + " " + stripped).strip()

        # Al acabar la pÃ¡gina, volcar el buffer restante
        if paragraph_buffer.strip():
            para = paragraph_buffer.strip()
            current_chunk, current_trimester = process_paragraph(
                para, current_section, current_trimester, page_num, current_chunk
            )
            current_page = page_num

    # Volcar el Ãºltimo chunk pendiente
    flush_chunk(current_chunk, current_section, current_trimester, current_page)

    return chunks


def save_faiss_index(embeddings: np.ndarray, chunks: list[dict]) -> None:
    """
    Construye un Ã­ndice FAISS IndexFlatIP (producto interno â‰ˆ coseno con
    vectores normalizados) y lo guarda en disco junto con los metadatos.
    """
    os.makedirs(FAISS_DIR, exist_ok=True)

    # Normalizar vectores para que el producto interno equivalga a similitud coseno
    vectors = np.array(embeddings, dtype="float32")
    faiss.normalize_L2(vectors)

    index = faiss.IndexFlatIP(EMBEDDING_DIM)
    index.add(vectors)
    faiss.write_index(index, FAISS_INDEX_PATH)

    # Guardar metadatos (texto, pÃ¡gina, secciÃ³n, trimestre) indexados por posiciÃ³n
    metadata = []
    for i, chunk in enumerate(chunks):
        metadata.append({
            "id": f"chunk-{i}",
            "text": chunk["text"],
            "page": chunk["page"],
            "section": chunk["section"],
            "trimester": chunk.get("trimester", "General"),
        })
    with open(FAISS_METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ’¾ Ãndice FAISS guardado en {FAISS_DIR}/ ({index.ntotal} vectores)")


def load_faiss_index():
    """
    Carga el Ã­ndice FAISS y los metadatos desde disco.
    Retorna (index, metadata) o (None, None) si no existen los archivos.
    """
    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(FAISS_METADATA_PATH):
        return None, None

    index = faiss.read_index(FAISS_INDEX_PATH)
    with open(FAISS_METADATA_PATH, "r", encoding="utf-8") as f:
        metadata = json.load(f)

    return index, metadata


def query_faiss(question: str, top_k: int = 5):
    """Busca en el Ã­ndice FAISS local. Retorna resultados o None si no hay Ã­ndice."""
    index, metadata = load_faiss_index()
    if index is None:
        return None

    model = SentenceTransformer(EMBEDDING_MODEL)
    q_embedding = model.encode(question)
    q_vector = np.array([q_embedding], dtype="float32")
    faiss.normalize_L2(q_vector)

    scores, indices = index.search(q_vector, min(top_k, index.ntotal))

    results = []
    for score, idx in zip(scores[0], indices[0]):
        if idx == -1:
            continue
        meta = metadata[idx]
        results.append({
            "score": float(score),
            "id": meta["id"],
            "metadata": {
                "text": meta["text"],
                "page": meta["page"],
                "section": meta["section"],
                "trimester": meta.get("trimester", "General"),
            },
        })
    return results


def sync():
    """
    Descarga todos los vectores de Pinecone y reconstruye el Ã­ndice FAISS local.
    Ãštil cuando Pinecone fue actualizado desde otro entorno.
    """
    api_key = load_env()

    print("ğŸŒ² Conectando con Pinecone...")
    pc = Pinecone(api_key=api_key)
    index = pc.Index(INDEX_NAME)

    # Obtener estadÃ­sticas del Ã­ndice
    stats = index.describe_index_stats()
    total_vectors = stats.total_vector_count
    if total_vectors == 0:
        print("âš ï¸  El Ã­ndice de Pinecone estÃ¡ vacÃ­o. Ejecuta 'ingest' primero.")
        return

    print(f"   â†’ {total_vectors} vectores en Pinecone.")

    # Descargar todos los vectores de Pinecone usando list + fetch
    print("ğŸ“¥ Descargando vectores de Pinecone...")
    all_ids = []
    for ids_batch in index.list():
        all_ids.extend(ids_batch)

    fetched = index.fetch(ids=all_ids)
    vectors_dict = fetched.vectors

    # Ordenar por ID para mantener consistencia
    sorted_ids = sorted(vectors_dict.keys(), key=lambda x: int(x.split("-")[1]))

    embeddings = []
    chunks = []
    for vid in sorted_ids:
        vec_data = vectors_dict[vid]
        embeddings.append(vec_data.values)
        chunks.append({
            "text": vec_data.metadata.get("text", ""),
            "page": vec_data.metadata.get("page", 0),
            "section": vec_data.metadata.get("section", ""),
            "trimester": vec_data.metadata.get("trimester", "General"),
        })

    embeddings_np = np.array(embeddings, dtype="float32")
    save_faiss_index(embeddings_np, chunks)
    print(f"\nâœ… SincronizaciÃ³n completada: {len(chunks)} vectores descargados y guardados en FAISS.")


def create_or_get_index(pc: Pinecone) -> None:
    """Crea el Ã­ndice en Pinecone si no existe y espera a que estÃ© listo."""
    existing = [idx.name for idx in pc.list_indexes()]
    if INDEX_NAME not in existing:
        print(f"   â†’ Creando Ã­ndice '{INDEX_NAME}' ({EMBEDDING_DIM} dimensiones)...")
        pc.create_index(
            name=INDEX_NAME,
            dimension=EMBEDDING_DIM,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1"),
        )
        while not pc.describe_index(INDEX_NAME).status["ready"]:
            print("   â³ Esperando a que el Ã­ndice estÃ© listo...")
            time.sleep(2)
        print(f"   âœ… Ãndice creado.")
    else:
        print(f"   â„¹ï¸  El Ã­ndice '{INDEX_NAME}' ya existe.")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ingesta
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ingest():
    """Lee el PDF, genera embeddings y los sube a Pinecone."""
    api_key = load_env()

    # 1. Leer PDF
    print(f"ğŸ“„ Leyendo PDF: {PDF_PATH}")
    pages = extract_text_from_pdf(PDF_PATH)
    print(f"   â†’ {len(pages)} pÃ¡ginas con texto.")

    # 2. Chunking semÃ¡ntico
    print(f"âœ‚ï¸  Aplicando chunking semÃ¡ntico (max={MAX_CHUNK_CHARS} chars por chunk)...")
    chunks = split_into_chunks(pages)
    print(f"   â†’ {len(chunks)} fragmentos generados.")

    # Mostrar resumen de secciones detectadas
    sections = sorted(set(c["section"] for c in chunks))
    print(f"   â†’ {len(sections)} secciones detectadas: {', '.join(sections)}")

    # 3. Generar embeddings con HuggingFace
    print(f"ğŸ¤— Cargando modelo de embeddings: {EMBEDDING_MODEL}")
    model = SentenceTransformer(EMBEDDING_MODEL)
    texts = [c["text"] for c in chunks]
    print(f"ğŸ”¢ Generando embeddings para {len(texts)} fragmentos...")
    embeddings = model.encode(texts, show_progress_bar=True, batch_size=64)

    # 4. Conectar con Pinecone y crear Ã­ndice
    print(f"ğŸŒ² Conectando con Pinecone...")
    pc = Pinecone(api_key=api_key)
    create_or_get_index(pc)
    index = pc.Index(INDEX_NAME)

    # 5. Borrar vectores anteriores para evitar duplicados en reingesta
    print(f"ğŸ—‘ï¸  Limpiando Ã­ndice anterior...")
    index.delete(delete_all=True)

    # 6. Subir vectores en lotes
    print(f"ğŸš€ Subiendo vectores a Pinecone...")
    batch_size = 100
    for i in range(0, len(chunks), batch_size):
        batch = []
        for j in range(i, min(i + batch_size, len(chunks))):
            vector_id = f"chunk-{j}"
            metadata = {
                "text":      chunks[j]["text"],
                "page":      chunks[j]["page"],
                "section":   chunks[j]["section"],
                "trimester": chunks[j].get("trimester", "General"),
            }
            batch.append((vector_id, embeddings[j].tolist(), metadata))
        index.upsert(vectors=batch)
        print(f"   â†’ Subidos {min(i + batch_size, len(chunks))}/{len(chunks)}")

    # 7. Guardar Ã­ndice FAISS local
    print(f"ğŸ’¾ Construyendo Ã­ndice FAISS local...")
    save_faiss_index(embeddings, chunks)

    print(f"\nâœ… Ingesta completada: {len(chunks)} fragmentos en el Ã­ndice '{INDEX_NAME}' (Pinecone + FAISS local).")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Consulta
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def query(question: str, top_k: int = 5, engine: str = "auto"):
    """
    Busca los fragmentos mÃ¡s relevantes para una pregunta.

    Motores de bÃºsqueda:
      - 'faiss':    BÃºsqueda local con FAISS (rÃ¡pida, offline).
      - 'pinecone': BÃºsqueda en Pinecone (cloud).
      - 'auto':     Intenta FAISS primero, fallback a Pinecone.
    """
    used_engine = engine

    if engine in ("faiss", "auto"):
        faiss_results = query_faiss(question, top_k)
        if faiss_results is not None:
            used_engine = "faiss"
            print(f"\nğŸ” Pregunta: {question}")
            print(f"âš¡ Motor: FAISS (local)")
            print(f"ğŸ“Š Top {top_k} resultados:\n")
            for i, match in enumerate(faiss_results, 1):
                score     = match["score"]
                text      = match["metadata"]["text"]
                page      = match["metadata"]["page"]
                section   = match["metadata"].get("section", "â€”")
                trimester = match["metadata"].get("trimester", "â€”")
                print(f"  [{i}] (similitud: {score:.4f}) â€” {section} | {trimester} | PÃ¡gina {page}")
                print(f"      {text[:300]}...")
                print()
            return faiss_results
        elif engine == "faiss":
            print("âŒ No se encontrÃ³ Ã­ndice FAISS local. Ejecuta 'ingest' o 'sync' primero.")
            sys.exit(1)
        else:
            print("âš ï¸  No hay Ã­ndice FAISS local, usando Pinecone como fallback...")

    # BÃºsqueda con Pinecone
    api_key = load_env()
    model = SentenceTransformer(EMBEDDING_MODEL)
    q_embedding = model.encode(question).tolist()

    pc = Pinecone(api_key=api_key)
    index = pc.Index(INDEX_NAME)

    results = index.query(vector=q_embedding, top_k=top_k, include_metadata=True)

    print(f"\nğŸ” Pregunta: {question}")
    print(f"ğŸŒ² Motor: Pinecone (cloud)")
    print(f"ğŸ“Š Top {top_k} resultados:\n")
    for i, match in enumerate(results["matches"], 1):
        score     = match["score"]
        text      = match["metadata"]["text"]
        page      = match["metadata"]["page"]
        section   = match["metadata"].get("section", "â€”")
        trimester = match["metadata"].get("trimester", "â€”")
        print(f"  [{i}] (similitud: {score:.4f}) â€” {section} | {trimester} | PÃ¡gina {page}")
        print(f"      {text[:300]}...")
        print()

    return results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Main
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Uso:")
        print("  python main.py ingest                              â†’ Sube el PDF a Pinecone + FAISS local")
        print('  python main.py query "tu pregunta"                 â†’ Busca (FAISS local â†’ Pinecone fallback)')
        print('  python main.py query "tu pregunta" --engine faiss  â†’ Forzar bÃºsqueda en FAISS')
        print('  python main.py query "tu pregunta" --engine pinecone â†’ Forzar bÃºsqueda en Pinecone')
        print("  python main.py sync                                â†’ Sincroniza Pinecone â†’ FAISS local")
        sys.exit(0)

    command = sys.argv[1]

    if command == "ingest":
        ingest()
    elif command == "query":
        if len(sys.argv) < 3:
            print("âŒ Debes proporcionar una pregunta. Ejemplo:")
            print('   python main.py query "Â¿QuÃ© saberes bÃ¡sicos hay en matemÃ¡ticas?"')
            sys.exit(1)

        # Detectar flag --engine
        engine = "auto"
        args = sys.argv[2:]
        if "--engine" in args:
            idx = args.index("--engine")
            if idx + 1 < len(args):
                engine = args[idx + 1]
                if engine not in ("faiss", "pinecone", "auto"):
                    print(f"âŒ Motor desconocido: {engine}. Usa 'faiss', 'pinecone' o 'auto'.")
                    sys.exit(1)
                args = args[:idx] + args[idx + 2:]
            else:
                print("âŒ Debes especificar un motor despuÃ©s de --engine (faiss, pinecone, auto).")
                sys.exit(1)

        question = " ".join(args)
        query(question, engine=engine)
    elif command == "sync":
        sync()
    else:
        print(f"âŒ Comando desconocido: {command}")
        print("   Usa 'ingest', 'query' o 'sync'")

